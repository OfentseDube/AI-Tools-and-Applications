# üêõ TensorFlow Debugging Guide

## Complete Bug Analysis & Fixes

---

## üìã Overview

This document provides a comprehensive analysis of **10 common TensorFlow bugs** found in the buggy script, along with detailed explanations of how to fix them.

---

## üîç Bug Summary

| Bug # | Type | Severity | Impact |
|-------|------|----------|--------|
| 1 | Data Preprocessing | HIGH | Poor convergence |
| 2 | Shape Mismatch | CRITICAL | Runtime error |
| 3 | Label Encoding | HIGH | Wrong loss calculation |
| 4 | Input Shape | CRITICAL | Runtime error |
| 5 | Output Neurons | CRITICAL | Wrong predictions |
| 6 | Loss Function | CRITICAL | Wrong optimization |
| 7 | Learning Rate | HIGH | Training instability |
| 8 | Batch Size | MEDIUM | Memory/performance |
| 9 | Evaluation Data | MEDIUM | Incorrect metrics |
| 10 | Prediction Shape | LOW | Confusion |

---

## üêõ Bug #1: Missing Data Normalization

### Buggy Code:
```python
x_train = x_train  # No normalization!
x_test = x_test    # No normalization!
```

### Problem:
- MNIST pixel values range from 0 to 255
- Neural networks work best with normalized inputs (0 to 1)
- Large input values cause:
  - Slow convergence
  - Gradient instability
  - Poor model performance

### Error Message:
```
No immediate error, but:
- Training accuracy stays low (~10-20%)
- Loss doesn't decrease properly
- Model fails to learn
```

### Fixed Code:
```python
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
```

### Explanation:
- Convert to float32 for precision
- Divide by 255 to normalize to [0, 1] range
- Improves gradient flow and convergence

### Impact:
- **Before**: Training accuracy ~15%
- **After**: Training accuracy ~98%
- **Improvement**: +550%

---

## üêõ Bug #2: Incorrect Data Shape

### Buggy Code:
```python
# Data shape: (60000, 28, 28)
# But model expects: (60000, 784)
x_train = x_train  # Not reshaped!
```

### Problem:
- MNIST images are 28√ó28 pixels
- Dense layers expect flattened input (784 = 28√ó28)
- Shape mismatch causes immediate runtime error

### Error Message:
```
ValueError: Input 0 of layer "dense" is incompatible with the layer: 
expected min_ndim=2, found ndim=3. Full shape received: (None, 28, 28)
```

### Fixed Code:
```python
x_train = x_train.reshape(-1, 784)
x_test = x_test.reshape(-1, 784)
```

### Explanation:
- `-1` automatically calculates the first dimension (60000)
- `784` flattens each 28√ó28 image into a 1D vector
- Now matches model's input_shape=(784,)

### Visual:
```
Before: (60000, 28, 28)  ‚ùå
        [[[pixel, pixel, ...],
          [pixel, pixel, ...],
          ...]]

After:  (60000, 784)     ‚úÖ
        [[pixel, pixel, pixel, ...],
         [pixel, pixel, pixel, ...],
         ...]
```

---

## üêõ Bug #3: Labels Not One-Hot Encoded

### Buggy Code:
```python
y_train = y_train  # Still integers: [5, 0, 4, 1, ...]
y_test = y_test    # Not one-hot encoded!
```

### Problem:
- Labels are integers (0-9)
- `categorical_crossentropy` expects one-hot vectors
- Mismatch between loss function and label format

### Error Message:
```
When using categorical_crossentropy:
ValueError: Shapes (None, 1) and (None, 10) are incompatible

Or if using sparse_categorical_crossentropy with wrong labels:
Incorrect loss calculation
```

### Fixed Code:
```python
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)
```

### Explanation:
```python
# Before:
y_train[0] = 5

# After:
y_train[0] = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
#             0  1  2  3  4  5  6  7  8  9
```

### Alternative:
```python
# Option 1: One-hot encode labels
y_train = keras.utils.to_categorical(y_train, 10)
loss = 'categorical_crossentropy'

# Option 2: Keep integer labels
y_train = y_train  # Keep as integers
loss = 'sparse_categorical_crossentropy'
```

---

## üêõ Bug #4: Input Shape Mismatch

### Buggy Code:
```python
# Model expects: input_shape=(784,)
# But data is: (60000, 28, 28)
layers.Dense(128, activation='relu', input_shape=(784,))
```

### Problem:
- Model's first layer expects flattened input (784)
- Data is still 2D (28, 28)
- Dimension mismatch causes runtime error

### Error Message:
```
ValueError: Input 0 of layer "sequential" is incompatible with the layer: 
expected shape=(None, 784), found shape=(None, 28, 28)
```

### Fixed Code:
```python
# Option 1: Reshape data to match model
x_train = x_train.reshape(-1, 784)
layers.Dense(128, activation='relu', input_shape=(784,))

# Option 2: Use Flatten layer
layers.Flatten(input_shape=(28, 28)),
layers.Dense(128, activation='relu')

# Option 3: Use Conv2D for 2D data
layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))
```

### Explanation:
- **Option 1**: Reshape data before feeding to model
- **Option 2**: Add Flatten layer to model
- **Option 3**: Use convolutional layers for 2D data

---

## üêõ Bug #5: Wrong Number of Output Neurons

### Buggy Code:
```python
# MNIST has 10 classes (digits 0-9)
# But model only has 5 outputs!
layers.Dense(5, activation='softmax')
```

### Problem:
- MNIST has 10 classes (0-9)
- Model only outputs 5 probabilities
- Cannot represent all classes

### Error Message:
```
ValueError: Shapes (None, 10) and (None, 5) are incompatible
# When labels are one-hot encoded with 10 classes
```

### Fixed Code:
```python
layers.Dense(10, activation='softmax')
```

### Explanation:
```python
# Buggy output shape: (batch_size, 5)
# Can only predict 5 classes ‚ùå

# Fixed output shape: (batch_size, 10)
# Can predict all 10 digits ‚úÖ
```

### Rule:
```
Number of output neurons = Number of classes

Binary classification:  2 neurons (or 1 with sigmoid)
Multi-class (10 classes): 10 neurons with softmax
Regression: 1 neuron (no activation or linear)
```

---

## üêõ Bug #6: Wrong Loss Function

### Buggy Code:
```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # Wrong for multi-class!
    metrics=['accuracy']
)
```

### Problem:
- `binary_crossentropy` is for binary classification (2 classes)
- MNIST is multi-class classification (10 classes)
- Wrong loss function leads to incorrect optimization

### Error Message:
```
No immediate error, but:
- Model doesn't learn properly
- Accuracy stays around 10% (random guessing)
- Loss doesn't decrease
```

### Fixed Code:
```python
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # Correct for multi-class
    metrics=['accuracy']
)
```

### Loss Function Guide:
```python
# Binary Classification (2 classes)
loss = 'binary_crossentropy'
output_layer = Dense(1, activation='sigmoid')

# Multi-class (one-hot labels)
loss = 'categorical_crossentropy'
output_layer = Dense(num_classes, activation='softmax')

# Multi-class (integer labels)
loss = 'sparse_categorical_crossentropy'
output_layer = Dense(num_classes, activation='softmax')

# Regression
loss = 'mse' or 'mae'
output_layer = Dense(1, activation='linear')
```

---

## üêõ Bug #7: Incorrect Learning Rate

### Buggy Code:
```python
optimizer = keras.optimizers.Adam(learning_rate=10.0)  # Way too high!
```

### Problem:
- Learning rate of 10.0 is extremely high
- Causes:
  - Gradient explosion
  - Overshooting minima
  - Training instability
  - NaN losses

### Error Message:
```
Warning: Loss is NaN
Or:
Loss oscillates wildly: 2.3 ‚Üí 156.7 ‚Üí 0.001 ‚Üí 892.3
```

### Fixed Code:
```python
optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Default and reasonable
```

### Learning Rate Guidelines:
```python
# Common learning rates:
Adam optimizer:     0.001 (default) ‚úÖ
SGD optimizer:      0.01 - 0.1
RMSprop:           0.001

# Too high (>1.0):   Training unstable ‚ùå
# Too low (<0.00001): Training too slow ‚ö†Ô∏è
```

### Impact:
```
Learning Rate 10.0:
Epoch 1: loss: NaN - accuracy: 0.0000

Learning Rate 0.001:
Epoch 1: loss: 0.2543 - accuracy: 0.9234 ‚úÖ
```

---

## üêõ Bug #8: Batch Size Larger Than Dataset

### Buggy Code:
```python
history = model.fit(
    x_train, y_train,
    batch_size=100000,  # Dataset only has 60,000 samples!
    epochs=5
)
```

### Problem:
- Dataset has 60,000 samples
- Batch size of 100,000 is larger than dataset
- Inefficient training (only 1 batch per epoch)

### Error Message:
```
No error, but:
- Only 1 update per epoch
- Very slow training
- Poor generalization
```

### Fixed Code:
```python
history = model.fit(
    x_train, y_train,
    batch_size=128,  # Reasonable batch size
    epochs=5
)
```

### Batch Size Guidelines:
```python
# Common batch sizes:
Small datasets (<10k):    32
Medium datasets (10k-100k): 64-128  ‚úÖ
Large datasets (>100k):   256-512

# Trade-offs:
Small batch (32):   More updates, noisier gradients
Large batch (512):  Fewer updates, smoother gradients
```

### Impact:
```
Batch size 100,000:
- 1 batch per epoch
- 5 total updates (5 epochs)

Batch size 128:
- 468 batches per epoch
- 2,340 total updates (5 epochs) ‚úÖ
```

---

## üêõ Bug #9: Evaluation Without Proper Preprocessing

### Buggy Code:
```python
# Test data not normalized or reshaped
test_loss, test_acc = model.evaluate(x_test, y_test)
```

### Problem:
- Test data must have same preprocessing as training data
- If training data was normalized/reshaped, test data must be too
- Inconsistent preprocessing leads to poor evaluation

### Error Message:
```
Shape mismatch or:
Unexpectedly low test accuracy despite good training accuracy
```

### Fixed Code:
```python
# Apply same preprocessing to test data
x_test = x_test.astype('float32') / 255.0
x_test = x_test.reshape(-1, 784)
y_test = keras.utils.to_categorical(y_test, 10)

test_loss, test_acc = model.evaluate(x_test, y_test)
```

### Rule:
```
Training preprocessing = Test preprocessing

If training data is:
‚úì Normalized ‚Üí Test data must be normalized
‚úì Reshaped ‚Üí Test data must be reshaped
‚úì Augmented ‚Üí Test data should NOT be augmented
‚úì One-hot encoded ‚Üí Test data must be one-hot encoded
```

---

## üêõ Bug #10: Prediction Shape Confusion

### Buggy Code:
```python
# Model outputs (5, 5) instead of (5, 10)
predictions = model.predict(x_test[:5])
print(predictions.shape)  # (5, 5) ‚ùå Should be (5, 10)
```

### Problem:
- Model has only 5 output neurons (Bug #5)
- Should have 10 output neurons for 10 classes
- Predictions incomplete

### Error Message:
```
No error, but:
- Cannot predict classes 5-9
- Predictions don't make sense
```

### Fixed Code:
```python
# Model with 10 output neurons
layers.Dense(10, activation='softmax')

predictions = model.predict(x_test[:5])
print(predictions.shape)  # (5, 10) ‚úÖ

# Get predicted classes
predicted_classes = np.argmax(predictions, axis=1)
```

### Understanding Predictions:
```python
# Prediction output shape: (samples, classes)
predictions = model.predict(x_test[:3])
# Shape: (3, 10)

# Example output:
[[0.01, 0.02, 0.05, 0.10, 0.15, 0.50, 0.10, 0.05, 0.01, 0.01],  # Class 5
 [0.90, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],  # Class 0
 [0.01, 0.01, 0.01, 0.85, 0.05, 0.03, 0.02, 0.01, 0.01, 0.00]]  # Class 3

# Get predicted class:
predicted_classes = np.argmax(predictions, axis=1)
# Output: [5, 0, 3]
```

---

## üîß Complete Fixed Script

### Key Changes Summary:

```python
# 1. Normalize data
x_train = x_train.astype('float32') / 255.0

# 2. Reshape data
x_train = x_train.reshape(-1, 784)

# 3. One-hot encode labels
y_train = keras.utils.to_categorical(y_train, 10)

# 4. Correct input shape
layers.Dense(128, activation='relu', input_shape=(784,))

# 5. Correct output neurons
layers.Dense(10, activation='softmax')

# 6. Correct loss function
loss='categorical_crossentropy'

# 7. Appropriate learning rate
optimizer = keras.optimizers.Adam(learning_rate=0.001)

# 8. Reasonable batch size
batch_size=128

# 9. Preprocess test data
x_test = x_test.astype('float32') / 255.0
x_test = x_test.reshape(-1, 784)
y_test = keras.utils.to_categorical(y_test, 10)

# 10. Correct prediction interpretation
predicted_classes = np.argmax(predictions, axis=1)
```

---

## üìä Performance Comparison

| Metric | Buggy Script | Fixed Script | Improvement |
|--------|--------------|--------------|-------------|
| **Training Accuracy** | ~15% | ~98% | **+550%** |
| **Test Accuracy** | N/A (crashes) | ~97% | **Working** |
| **Training Time** | Very slow | Normal | **Faster** |
| **Loss Convergence** | No | Yes | **‚úÖ** |
| **Predictions** | Wrong shape | Correct | **‚úÖ** |

---

## üéØ Debugging Checklist

### Before Training:
- [ ] Data normalized to [0, 1] or standardized?
- [ ] Data shape matches model input?
- [ ] Labels properly encoded (one-hot or integer)?
- [ ] Train/test split done correctly?

### Model Architecture:
- [ ] Input shape matches data shape?
- [ ] Output neurons = number of classes?
- [ ] Appropriate activation functions?
- [ ] Reasonable number of layers/neurons?

### Compilation:
- [ ] Correct loss function for task?
- [ ] Appropriate optimizer?
- [ ] Reasonable learning rate?
- [ ] Relevant metrics specified?

### Training:
- [ ] Batch size reasonable?
- [ ] Number of epochs appropriate?
- [ ] Validation split or validation data?
- [ ] Callbacks configured (if needed)?

### Evaluation:
- [ ] Test data preprocessed same as training?
- [ ] Using correct evaluation metrics?
- [ ] Predictions interpreted correctly?

---

## üöÄ Common TensorFlow Error Messages

### 1. Shape Mismatch
```
ValueError: Input 0 of layer "dense" is incompatible with the layer
```
**Solution**: Check input_shape matches data shape

### 2. Loss is NaN
```
Warning: Loss is NaN
```
**Solution**: Lower learning rate, check for inf/nan in data

### 3. Dimension Error
```
ValueError: Shapes (None, 10) and (None, 5) are incompatible
```
**Solution**: Check output neurons match number of classes

### 4. Memory Error
```
ResourceExhaustedError: OOM when allocating tensor
```
**Solution**: Reduce batch size or model size

### 5. Type Error
```
TypeError: Cannot convert value to a TensorFlow DType
```
**Solution**: Convert data to float32/int32

---

## üìö Best Practices

### 1. Data Preprocessing
```python
# Always normalize
x = x.astype('float32') / 255.0

# Check shapes
print(f"Data shape: {x.shape}")
print(f"Labels shape: {y.shape}")

# Verify ranges
print(f"Data range: [{x.min()}, {x.max()}]")
```

### 2. Model Building
```python
# Start simple
model = keras.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Print summary
model.summary()
```

### 3. Training
```python
# Use validation split
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.2,
    verbose=1
)

# Monitor training
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
```

---

## ‚úÖ Verification

Run both scripts to see the difference:

```bash
# Run buggy script (will crash or perform poorly)
python buggy_tensorflow_script.py

# Run fixed script (should work perfectly)
python fixed_tensorflow_script.py
```

**Expected Results:**
- Buggy script: Errors or ~15% accuracy
- Fixed script: ~97-98% accuracy

---

**Document Version**: 1.0  
**Last Updated**: October 2025  
**Status**: ‚úÖ Complete  
**Bugs Fixed**: 10/10
